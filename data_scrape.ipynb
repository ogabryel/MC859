{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bedd89",
   "metadata": {},
   "source": [
    "# Coleta de vagas do Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2d1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdadda",
   "metadata": {},
   "source": [
    "**Funções auxiliares:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d18897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, verbose=False):\n",
    "    '''\n",
    "    Fetches a web page with retries and exponential backoff.\n",
    "    Args:\n",
    "        url (str): The URL of the web page to fetch.\n",
    "        verbose (bool): If True, prints status messages.\n",
    "    Returns:\n",
    "        str: The content of the web page, or None if failed.\n",
    "    '''\n",
    "    wait_time = 1\n",
    "\n",
    "    while True:\n",
    "        if wait_time > 60:\n",
    "            if verbose:\n",
    "                print(f\"Failed to fetch {url} after multiple attempts.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            # Raise an error for bad responses\n",
    "            response.raise_for_status()\n",
    "            if verbose:\n",
    "                print(f\"Successfully fetched {url}\")\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if verbose:\n",
    "                print(f\"Error fetching {url}: {e}. Retrying in {wait_time} seconds...\")\n",
    "\n",
    "            time.sleep(wait_time)\n",
    "            # Exponential backoff with a max wait time\n",
    "            wait_time *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64dbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_job_details(job_id, is_remote, verbose=False):\n",
    "    '''\n",
    "    Fetches job details from LinkedIn job posting.\n",
    "    Args:\n",
    "        job_id (str): The ID of the job to fetch.\n",
    "        is_remote (bool): Whether the job is remote.\n",
    "        verbose (bool): If True, prints status messages.\n",
    "    Returns:\n",
    "        dict: A dictionary containing job details, or None if failed.\n",
    "    '''\n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "\n",
    "    job_data = fetch_page(job_url, verbose)\n",
    "    # If this job could not be fetched, skip it\n",
    "    if not job_data:\n",
    "        return None\n",
    "\n",
    "    # The response is a HTML page, parse it\n",
    "    soup = BeautifulSoup(job_data, 'html.parser')\n",
    "\n",
    "    # Information about the job\n",
    "    job_name = soup.select_one('.top-card-layout__entity-info a')\n",
    "    job_location = soup.select_one('.topcard__flavor-row .topcard__flavor--bullet')\n",
    "    job_description = soup.select_one('.description__text')\n",
    "    job_remote = is_remote\n",
    "\n",
    "    # If anything is missing, skip this job\n",
    "    if not job_name or not job_location or not job_description:\n",
    "        if verbose:\n",
    "            print(f\"Skipping job {job_id} due to missing information.\")\n",
    "        return None\n",
    "\n",
    "    # Create a dictionary to hold job details\n",
    "    job_details = {\n",
    "        'id': job_id,\n",
    "        'name': job_name.get_text(strip=True),\n",
    "        'location': job_location.get_text(strip=True),\n",
    "        'description': job_description.get_text(strip=True),\n",
    "        'remote': job_remote,\n",
    "    }\n",
    "\n",
    "    return job_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_location = \"Brazil\"\n",
    "maximum_time = \"r86400\" # consider only jobs posted in the last 24 hours\n",
    "\n",
    "def fetch_jobs_with_keywords(keywords, verbose=False):\n",
    "    '''\n",
    "    Fetches job details from LinkedIn using a certain keyword. Considers both remote and non-remote jobs.\n",
    "    Args:\n",
    "        keywords (str): The keywords to search for.\n",
    "        verbose (bool): If True, prints status messages.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing job details, or an empty list if nothing was found.\n",
    "    '''\n",
    "    all_jobs = []\n",
    "\n",
    "    base_search_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "\n",
    "    keywords = keywords.replace(\" \", \"%20\")\n",
    "\n",
    "    # Fetch both remote and non-remote jobs\n",
    "    for is_remote in [False, True]:\n",
    "        if verbose:\n",
    "            print(f\"Fetching {'remote' if is_remote else 'non-remote'} jobs for keywords: {keywords}\")\n",
    "\n",
    "        # Add keyword, location, time parameter and remote filter\n",
    "        location = job_location\n",
    "        time = maximum_time\n",
    "        remote = \"2\" if is_remote else \"1%2C3\"\n",
    "\n",
    "        search_url = f\"{base_search_url}?keywords={keywords}&location={location}&f_TPR={time}&f_WT={remote}&start={{}}\"\n",
    "\n",
    "        pagination_index = 0\n",
    "        while True:\n",
    "            jobs = fetch_page(search_url.format(pagination_index), verbose)\n",
    "\n",
    "            # If there was an error fetching the page, stop the search\n",
    "            if not jobs:\n",
    "                break\n",
    "\n",
    "            # Parse the job listings\n",
    "            soup = BeautifulSoup(jobs, 'html.parser')\n",
    "\n",
    "            # Check if there are no jobs found\n",
    "            total_jobs = soup.find_all(\"li\")\n",
    "            if not total_jobs:\n",
    "                if verbose:\n",
    "                    print(f\"No more jobs found on page {pagination_index}.\")\n",
    "                break\n",
    "\n",
    "            # Extract job details for each job\n",
    "            for job in total_jobs:\n",
    "                job_id = job.find(\"div\", {\"class\": \"base-card\"}).get('data-entity-urn').split(\":\")[3] if job.find(\"div\", {\"class\": \"base-card\"}) else None\n",
    "\n",
    "                if job_id is None:\n",
    "                    # No more jobs to process\n",
    "                    break\n",
    "\n",
    "                # Only process if previous searches (with different keywords) didn't fetch this job\n",
    "                job_information = fetch_job_details(job_id, is_remote, verbose)\n",
    "\n",
    "                # Add job information if it was successfully fetched\n",
    "                if job_information:\n",
    "                    all_jobs.append(job_information)\n",
    "\n",
    "            # Move to the next page\n",
    "            pagination_index += len(total_jobs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total jobs fetched: {len(all_jobs)}\")\n",
    "\n",
    "    return all_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3db054",
   "metadata": {},
   "source": [
    "**Palavras-chave para as buscas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1f180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search keywords\n",
    "keywords = [\n",
    "    'Desenvolvedor',\n",
    "    'Programador',\n",
    "    'Software',\n",
    "    'Hardware',\n",
    "    'IA',\n",
    "    'Inteligência Artificial',\n",
    "    'Machine Learning',\n",
    "    'Data Science',\n",
    "    'Engenheiro de Software',\n",
    "    'Engenheiro de Dados',\n",
    "    'Desenvolvimento Web',\n",
    "    'Backend',\n",
    "    'Frontend',\n",
    "    'Full Stack',\n",
    "    'Cloud',\n",
    "    'DevOps',\n",
    "    'Big Data',\n",
    "    'QA',\n",
    "    'UX',\n",
    "    'UI',\n",
    "    'CI',\n",
    "    'CD',\n",
    "    'Android',\n",
    "    'iOS',\n",
    "    'Mobile',\n",
    "    'TI',\n",
    "    'Cibersegurança',\n",
    "    'Redes',\n",
    "    'Robótica',\n",
    "    'Jogos',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f0833",
   "metadata": {},
   "source": [
    "**Coleta de dados usando as palavras-chave definidas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7684a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3290 jobs\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "# Fetch jobs for each keyword and save everything to a CSV file\n",
    "all_jobs_data = []\n",
    "for keyword in keywords:\n",
    "    jobs = fetch_jobs_with_keywords(keyword, verbose)\n",
    "\n",
    "    if jobs:\n",
    "        all_jobs_data.extend(jobs)\n",
    "\n",
    "# Convert to a DataFrame, remove possible duplicates, and save as a CSV\n",
    "if all_jobs_data:\n",
    "    df = pd.DataFrame(all_jobs_data)\n",
    "    # Due to how Linkedin searches work, some jobs may appear multiple times if they match multiple keywords\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    csv_filename = f\"linkedinjobs_{datetime.today().strftime('%Y-%m-%d')}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Saved {len(all_jobs_data)} jobs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
