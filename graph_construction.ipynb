{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003f7e6b",
   "metadata": {},
   "source": [
    "# Construção do grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b820095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc15780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_jobs = pd.read_csv('merged_data.csv')\n",
    "technical_skills = pd.read_csv('technical_skills_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associando um novo UUID para cada job\n",
    "scraped_jobs['id'] = scraped_jobs['id'].astype('str')\n",
    "for i in range(scraped_jobs.shape[0]):\n",
    "    scraped_jobs.at[i, 'id'] = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54277100",
   "metadata": {},
   "source": [
    "## Pré-processamento das localizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60578410",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_region = {\n",
    "    # States to regions mapping\n",
    "    'Acre': 'North', 'Alagoas': 'Northeast', 'Amapá': 'North', 'Amazonas': 'North', 'Bahia': 'Northeast', 'Ceará': 'Northeast', 'Distrito Federal': 'Central-West', 'Espírito Santo': 'Southeast', 'Goiás': 'Central-West', 'Maranhão': 'Northeast', 'Mato Grosso': 'Central-West', 'Mato Grosso do Sul': 'Central-West', 'Minas Gerais': 'Southeast', 'Pará': 'North', 'Paraíba': 'Northeast', 'Paraná': 'South', 'Pernambuco': 'Northeast', 'Piauí': 'Northeast', 'Rio de Janeiro': 'Southeast', 'Rio Grande do Norte': 'Northeast', 'Rio Grande do Sul': 'South', 'Rondônia': 'North', 'Roraima': 'North', 'Santa Catarina': 'South', 'São Paulo': 'Southeast', 'Sergipe': 'Northeast', 'Tocantins': 'North',\n",
    "\n",
    "    # Other locations used in the dataset\n",
    "    'Federal District': 'Central-West', 'Belo Horizonte': 'Southeast', 'Porto Alegre': 'South', 'Curitiba': 'South', 'Campinas': 'Southeast', 'Ribeirão Preto': 'Southeast', 'Natal': 'Northeast', 'Recife': 'Northeast', 'Vitoria': 'Southeast', 'Londrina': 'South', 'Goiania': 'Central-West', 'Brasilia': 'Central-West', 'Salvador': 'Northeast', 'Florianopolis': 'South', 'Fortaleza': 'Northeast', 'Belem': 'North', 'Manaus': 'North', 'João Pessoa': 'Northeast', 'Cuiaba': 'Central-West',\n",
    "}\n",
    "\n",
    "# Remap locations to the region they belong to\n",
    "def map_location(location):\n",
    "    '''\n",
    "    Remaps a location to its corresponding region in Brazil.\n",
    "    Args:\n",
    "        location (str): The location string to be remapped.\n",
    "    Returns:\n",
    "        str: The region corresponding to the location, or the original location if not found.\n",
    "    '''\n",
    "    for state, region in location_to_region.items():\n",
    "        if state in location:\n",
    "            return region\n",
    "    return location\n",
    "\n",
    "scraped_jobs['location'] = scraped_jobs['location'].apply(map_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7cb993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions for non-remote jobs: ['Southeast' 'South' 'Northeast' 'Central-West' 'North']\n",
      "Regions for remote jobs: ['Brazil' 'Southeast' 'South' 'Northeast' 'Central-West' 'North'\n",
      " 'Latin America']\n"
     ]
    }
   ],
   "source": [
    "# Output all existing values in column location for non-remote jobs\n",
    "print(f\"Regions for non-remote jobs: {scraped_jobs[scraped_jobs['remote'] == False]['location'].unique()}\")\n",
    "\n",
    "# Output all existing values in column location for remote jobs\n",
    "print(f\"Regions for remote jobs: {scraped_jobs[scraped_jobs['remote'] == True]['location'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a5551",
   "metadata": {},
   "source": [
    "Todas as vagas não remotas tiveram sua localização identificada. Para vagas remotas, faz sentido manter \"Brasil\" se não houver sido especificado a localização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57625d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap \"Latin America\" to \"Brazil\" to maintain consistency\n",
    "scraped_jobs['location'] = scraped_jobs['location'].replace('Latin America', 'Brazil')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae316170",
   "metadata": {},
   "source": [
    "## Identificação das habilidades técnicas a partir do título e da descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a39eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_splitter(skill):\n",
    "    '''\n",
    "    Returns a regex pattern to split text into tokens, ensuring characters existing in the given skill are preserved.\n",
    "    Args:\n",
    "        skill (str): The skill string to determine which characters to preserve.\n",
    "    Returns:\n",
    "        str: The regex pattern for token splitting.\n",
    "    '''\n",
    "    # Find all non-letter characters in the skill\n",
    "    non_letters = set(re.findall(r\"[^a-zA-Z]\", skill))\n",
    "    escaped_non_letters = \"\".join([re.escape(c) for c in non_letters])\n",
    "    # Create a regex pattern that matches sequences of characters that are not letters, digits,\n",
    "    #    $, #, +, or any of the non-letter characters in the skill\n",
    "    # This ensures that characters like '+' in 'C++' are preserved as part of the token, and\n",
    "    #    avoids splitting 'C++' into 'C' and '' which could lead to false positives\n",
    "    token_splitter = re.compile(fr\"[^\\w\\$\\#\\+{escaped_non_letters}]+\")\n",
    "    return token_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d29e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(string):\n",
    "    '''\n",
    "    Removes all numeric characters from the input string.\n",
    "    Args:\n",
    "        string (str): The input string from which numbers should be removed.\n",
    "    Returns:\n",
    "        str: The input string with all numeric characters removed.\n",
    "    '''\n",
    "    return re.sub(r'\\d+', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db916e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(job_title, job_description):\n",
    "    '''\n",
    "    Extracts technical skills from job title and description. The skills are matched using exact matching for skills with less than 6 characters, and fuzzy matching (normalized Indel similarity) for skills with 6 or more characters.\n",
    "    Args:\n",
    "        job_title (str): The job title.\n",
    "        job_description (str): The job description.\n",
    "    Returns:\n",
    "        list: A list of extracted technical skills.\n",
    "    '''\n",
    "    found_technical_skills = set()\n",
    "\n",
    "    # Remove numbers (phone numbers, years of experience, framework versions, etc.) from title and description\n",
    "    job_title = remove_numbers(job_title)\n",
    "    job_description = remove_numbers(job_description)\n",
    "\n",
    "    title = job_title.lower()\n",
    "    description = job_description.lower()\n",
    "\n",
    "    for skill in technical_skills['skill']:\n",
    "        skill = skill.lower()\n",
    "        # If the skill contains less than 6 characters, use exact match\n",
    "        if len(skill) < 6:\n",
    "            # Split title and description by non-alphanumeric characters not present in the skill\n",
    "            token_splitter = get_token_splitter(skill)\n",
    "            split_title = re.split(token_splitter, title)\n",
    "            split_description = re.split(token_splitter, description)\n",
    "\n",
    "            if skill in split_description or skill in split_title:\n",
    "                found_technical_skills.add(skill)\n",
    "        else:\n",
    "            # Use fuzzy matching (normalized Indel similarity) for skills with 6 or more characters\n",
    "            minimum_match_score = int(100 * len(skill) / (len(skill) + 1)) # At most 1 insertion/deletion\n",
    "\n",
    "            # Don't consider matches if the skill is longer than the text being searched\n",
    "            if len(skill) <= len(description) and rapidfuzz.fuzz.partial_ratio(skill, description) > minimum_match_score:\n",
    "                found_technical_skills.add(skill)\n",
    "            elif len(skill) <= len(title) and rapidfuzz.fuzz.partial_ratio(skill, title) > minimum_match_score:\n",
    "                found_technical_skills.add(skill)\n",
    "\n",
    "    return list(found_technical_skills) if found_technical_skills else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e5bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all entries, total done 100%                                         \n"
     ]
    }
   ],
   "source": [
    "map_skills = True\n",
    "\n",
    "if map_skills:\n",
    "    # Create a new column 'found_skills' to store the extracted skills before the graph is constructed\n",
    "    scraped_jobs['found_skills'] = np.empty((scraped_jobs.shape[0], 0)).tolist()\n",
    "\n",
    "    try:\n",
    "        for i in range(scraped_jobs.shape[0]):\n",
    "            # Add to the column 'found_skills' the extracted skills from the job title and description\n",
    "            scraped_jobs.at[i, 'found_skills'] = extract_skills(scraped_jobs['name'].iloc[i], scraped_jobs['description'].iloc[i])\n",
    "\n",
    "            if (i % 100) == 0:\n",
    "                print(f\"\\rProcessed entries up to {i}, total done {i / scraped_jobs.shape[0]}\".ljust(80), end='')\n",
    "\n",
    "        print(f\"\\rProcessed all entries, total done 100%\".ljust(80))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted.\".ljust(80))\n",
    "\n",
    "    # Save the dataframe with the new column to a new CSV file\n",
    "    scraped_jobs.to_csv('scraped_jobs_with_skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cefb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 50 random samples of the dataframe to a new CSV file for manual verification\n",
    "scraped_jobs.sample(n=50, random_state=random.seed(datetime.now().timestamp())).to_csv('scraped_jobs_with_skills_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0651a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from scraped_jobs rows where found_skills is empty\n",
    "# Even though related keywords were used in the scraping process,\n",
    "#    some jobs returned by the API are completely unrelated to technology\n",
    "scraped_jobs = scraped_jobs[scraped_jobs['found_skills'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a72474",
   "metadata": {},
   "source": [
    "## Construção do Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dfe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_csv = False\n",
    "\n",
    "if read_from_csv:\n",
    "    scraped_jobs = pd.read_csv('scraped_jobs_with_skills.csv')\n",
    "    scraped_jobs['found_skills'] = scraped_jobs['found_skills'].apply(ast.literal_eval)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Create edges between jobs based on skills\n",
    "for _, row in scraped_jobs.iterrows():\n",
    "    # Create a new node for this job\n",
    "    G.add_node(row['id'], type=\"job\", location=row['location'], remote=row['remote'])\n",
    "    for skill in row['found_skills']:\n",
    "        # Create a new node for the skill if it doesn't exist\n",
    "        if not G.has_node(skill):\n",
    "            G.add_node(skill, type=\"skill\")\n",
    "        # Create an edge between the job and the skill\n",
    "        G.add_edge(row['id'], skill)\n",
    "\n",
    "# Save as GEXF\n",
    "nx.write_gexf(G, 'job_skill_graph.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
